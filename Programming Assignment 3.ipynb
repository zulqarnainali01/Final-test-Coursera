{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "Welcome to Assignment 3. This will be even more fun. Now we will calculate statistical measures. \n",
    "\n",
    "## You only have to pass 4 out of 7 functions\n",
    "\n",
    "Just make sure you hit the play button on each cell from top to down. There are seven functions you have to implement. Please also make sure than on each change on a function you hit the play button again on the corresponding cell to make it available to the rest of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to run in a IBM Watson Studio default runtime (NOT the Watson Studio Apache Spark Runtime as the default runtime with 1 vCPU is free of charge). Therefore, we install Apache Spark in local mode for test purposes only. Please don't use it in production.\n",
    "\n",
    "In case you are facing issues, please read the following two documents first:\n",
    "\n",
    "https://github.com/IBM/skillsnetwork/wiki/Environment-Setup\n",
    "\n",
    "https://github.com/IBM/skillsnetwork/wiki/FAQ\n",
    "\n",
    "Then, please feel free to ask:\n",
    "\n",
    "https://coursera.org/learn/machine-learning-big-data-apache-spark/discussions/all\n",
    "\n",
    "Please make sure to follow the guidelines before asking a question:\n",
    "\n",
    "https://github.com/IBM/skillsnetwork/wiki/FAQ#im-feeling-lost-and-confused-please-help-me\n",
    "\n",
    "\n",
    "If running outside Watson Studio, this should work as well. In case you are running in an Apache Spark context outside Watson Studio, please remove the Apache Spark setup in the first notebook cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n",
    "\n",
    "\n",
    "if ('sc' in locals() or 'sc' in globals()):\n",
    "    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==2.4.5 in /usr/local/python/3.10.4/lib/python3.10/site-packages (2.4.5)\n",
      "Requirement already satisfied: py4j==0.10.7 in /usr/local/python/3.10.4/lib/python3.10/site-packages (from pyspark==2.4.5) (0.10.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==2.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bytes' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkContext, SparkConf\n\u001b[1;32m      3\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[1;32m      4\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/__init__.py:51\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtypes\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconf\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkConf\n\u001b[0;32m---> 51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontext\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkContext\n\u001b[1;32m     52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrdd\u001b[39;00m \u001b[39mimport\u001b[39;00m RDD, RDDBarrier\n\u001b[1;32m     53\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfiles\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkFiles\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/context.py:31\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtempfile\u001b[39;00m \u001b[39mimport\u001b[39;00m NamedTemporaryFile\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpy4j\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotocol\u001b[39;00m \u001b[39mimport\u001b[39;00m Py4JError\n\u001b[0;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m accumulators\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39maccumulators\u001b[39;00m \u001b[39mimport\u001b[39;00m Accumulator\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbroadcast\u001b[39;00m \u001b[39mimport\u001b[39;00m Broadcast, BroadcastPickleRegistry\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/accumulators.py:97\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39msocketserver\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mSocketServer\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mthreading\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserializers\u001b[39;00m \u001b[39mimport\u001b[39;00m read_int, PickleSerializer\n\u001b[1;32m    100\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mAccumulator\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAccumulatorParam\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    103\u001b[0m pickleSer \u001b[39m=\u001b[39m PickleSerializer()\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/serializers.py:72\u001b[0m\n\u001b[1;32m     69\u001b[0m     protocol \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m     70\u001b[0m     xrange \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m\n\u001b[0;32m---> 72\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m cloudpickle\n\u001b[1;32m     73\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m _exception_message\n\u001b[1;32m     76\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mPickleSerializer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMarshalSerializer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mUTF8Deserializer\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/cloudpickle.py:145\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m         \u001b[39mreturn\u001b[39;00m types\u001b[39m.\u001b[39mCodeType(\n\u001b[1;32m    127\u001b[0m             co\u001b[39m.\u001b[39mco_argcount,\n\u001b[1;32m    128\u001b[0m             co\u001b[39m.\u001b[39mco_kwonlyargcount,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m             (),\n\u001b[1;32m    142\u001b[0m         )\n\u001b[0;32m--> 145\u001b[0m _cell_set_template_code \u001b[39m=\u001b[39m _make_cell_set_template_code()\n\u001b[1;32m    148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcell_set\u001b[39m(cell, value):\n\u001b[1;32m    149\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Set the value of a closure cell.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/cloudpickle.py:126\u001b[0m, in \u001b[0;36m_make_cell_set_template_code\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[39mreturn\u001b[39;00m types\u001b[39m.\u001b[39mCodeType(\n\u001b[1;32m    110\u001b[0m         co\u001b[39m.\u001b[39mco_argcount,\n\u001b[1;32m    111\u001b[0m         co\u001b[39m.\u001b[39mco_nlocals,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m         (),\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    125\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m types\u001b[39m.\u001b[39;49mCodeType(\n\u001b[1;32m    127\u001b[0m         co\u001b[39m.\u001b[39;49mco_argcount,\n\u001b[1;32m    128\u001b[0m         co\u001b[39m.\u001b[39;49mco_kwonlyargcount,\n\u001b[1;32m    129\u001b[0m         co\u001b[39m.\u001b[39;49mco_nlocals,\n\u001b[1;32m    130\u001b[0m         co\u001b[39m.\u001b[39;49mco_stacksize,\n\u001b[1;32m    131\u001b[0m         co\u001b[39m.\u001b[39;49mco_flags,\n\u001b[1;32m    132\u001b[0m         co\u001b[39m.\u001b[39;49mco_code,\n\u001b[1;32m    133\u001b[0m         co\u001b[39m.\u001b[39;49mco_consts,\n\u001b[1;32m    134\u001b[0m         co\u001b[39m.\u001b[39;49mco_names,\n\u001b[1;32m    135\u001b[0m         co\u001b[39m.\u001b[39;49mco_varnames,\n\u001b[1;32m    136\u001b[0m         co\u001b[39m.\u001b[39;49mco_filename,\n\u001b[1;32m    137\u001b[0m         co\u001b[39m.\u001b[39;49mco_name,\n\u001b[1;32m    138\u001b[0m         co\u001b[39m.\u001b[39;49mco_firstlineno,\n\u001b[1;32m    139\u001b[0m         co\u001b[39m.\u001b[39;49mco_lnotab,\n\u001b[1;32m    140\u001b[0m         co\u001b[39m.\u001b[39;49mco_cellvars,  \u001b[39m# this is the trickery\u001b[39;49;00m\n\u001b[1;32m    141\u001b[0m         (),\n\u001b[1;32m    142\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bytes' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkContext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39mgetOrCreate(SparkConf()\u001b[39m.\u001b[39msetMaster(\u001b[39m\"\u001b[39m\u001b[39mlocal[*]\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m spark \u001b[39m=\u001b[39m SparkSession \\\n\u001b[1;32m      4\u001b[0m     \u001b[39m.\u001b[39mbuilder \\\n\u001b[1;32m      5\u001b[0m     \u001b[39m.\u001b[39mgetOrCreate()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SparkContext' is not defined"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All functions can be implemented using DataFrames, ApacheSparkSQL or RDDs. We are only interested in the result. You are given the reference to the data frame in the \"df\" parameter and in case you want to use SQL just use the \"spark\" parameter which is a reference to the global SparkSession object. Finally if you want to use RDDs just use \"df.rdd\" for obtaining a reference to the underlying RDD object. But we discurage using RDD at this point in time.\n",
    "\n",
    "Let's start with the first function. Please calculate the minimal temperature for the test data set you have created. We've provided a little skeleton for you in case you want to use SQL. Everything can be implemented using SQL only if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minTemperature(df,spark):\n",
    "    #TODO Please enter your code here, you are not required to use the template code below\n",
    "    #some reference: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "    return spark.sql(\"SELECT MIN(temperature) as mintemp from washing\").first().mintemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please now do the same for the mean of the temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanTemperature(df,spark):\n",
    "    #TODO Please enter your code here, you are not required to use the template code below\n",
    "    #some reference: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "    return spark.sql(\"SELECT AVG(temperature) as meantemp from washing\").first().meantemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please now do the same for the maximum of the temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxTemperature(df,spark):\n",
    "    #TODO Please enter your code here, you are not required to use the template code below\n",
    "    #some reference: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "    return spark.sql(\"SELECT MAX(temperature) as maxtemp from washing\").first().maxtemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please now do the same for the standard deviation of the temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdTemperature(df,spark):\n",
    "    #TODO Please enter your code here, you are not required to use the template code below\n",
    "    #some reference: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "    #https://spark.apache.org/docs/2.3.0/api/sql/\n",
    "    return spark.sql(\"SELECT STDDEV(temperature) as sdtemp from washing\").first().sdtemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please now do the same for the skew of the temperature. Since the SQL statement for this is a bit more complicated we've provided a skeleton for you. You have to insert custom code at four positions in order to make the function work. Alternatively you can also remove everything and implement if on your own. Note that we are making use of two previously defined functions, so please make sure they are correct. Also note that we are making use of python's string formatting capabilitis where the results of the two function calls to \"meanTemperature\" and \"sdTemperature\" are inserted at the \"%s\" symbols in the SQL string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewTemperature(df,spark):    \n",
    "    return spark.sql(\"\"\"\n",
    "SELECT \n",
    "    (\n",
    "        count(temperature)/(count(temperature)-1)/(count(temperature)-2)\n",
    "    ) *\n",
    "    SUM (\n",
    "        POWER(temperature-%s,3)/POWER(%s,3)\n",
    "    )\n",
    "\n",
    "as sktemperature from washing\n",
    "                    \"\"\" %(meanTemperature(df,spark),sdTemperature(df,spark))).first().sktemperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurtosis is the 4th statistical moment, so if you are smart you can make use of the code for skew which is the 3rd statistical moment. Actually only two things are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kurtosisTemperature(df,spark):    \n",
    "        return spark.sql(\"\"\"\n",
    "SELECT \n",
    "    (\n",
    "        1/COUNT(temperature)\n",
    "    ) *\n",
    "    SUM (\n",
    "        POWER(temperature-%s,4)/POWER(%s,4)\n",
    "    )\n",
    "as ktemperature from washing\n",
    "                    \"\"\" %(meanTemperature(df,spark),sdTemperature(df,spark))).first().ktemperature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a hint. This can be solved easily using SQL as well, but as shown in the lecture also using RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlationTemperatureHardness(df,spark):\n",
    "    #TODO Please enter your code here, you are not required to use the template code below\n",
    "    #some reference: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "    #https://spark.apache.org/docs/2.3.0/api/sql/\n",
    "    return spark.sql(\"SELECT corr(temperature,hardness) as temperaturehardness from washing\").first().temperaturehardness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to grab a PARQUET file and create a dataframe out of it. Using SparkSQL you can handle it like a database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/IBM/coursera/blob/master/coursera_ds/washing.parquet?raw=true\n",
    "!mv washing.parquet?raw=true washing.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('washing.parquet')\n",
    "df.createOrReplaceTempView('washing')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test the functions you've implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_temperature = 0\n",
    "mean_temperature = 0\n",
    "max_temperature = 0\n",
    "sd_temperature = 0\n",
    "skew_temperature = 0\n",
    "kurtosis_temperature = 0\n",
    "correlation_temperature = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_temperature = minTemperature(df,spark)\n",
    "print(min_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mean_temperature \u001b[39m=\u001b[39m meanTemperature(df,spark)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(mean_temperature)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "mean_temperature = meanTemperature(df,spark)\n",
    "print(mean_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m max_temperature \u001b[39m=\u001b[39m maxTemperature(df,spark)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(max_temperature)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "max_temperature = maxTemperature(df,spark)\n",
    "print(max_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sd_temperature \u001b[39m=\u001b[39m sdTemperature(df,spark)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(sd_temperature)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "sd_temperature = sdTemperature(df,spark)\n",
    "print(sd_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m skew_temperature \u001b[39m=\u001b[39m skewTemperature(df,spark)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(skew_temperature)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "skew_temperature = skewTemperature(df,spark)\n",
    "print(skew_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kurtosis_temperature \u001b[39m=\u001b[39m kurtosisTemperature(df,spark)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(kurtosis_temperature)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "kurtosis_temperature = kurtosisTemperature(df,spark)\n",
    "print(kurtosis_temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m correlation_temperature \u001b[39m=\u001b[39m correlationTemperatureHardness(df,spark)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(correlation_temperature)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "correlation_temperature = correlationTemperatureHardness(df,spark)\n",
    "print(correlation_temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you are done, please submit this notebook to the grader. \n",
    "We have to install a little library in order to submit to coursera first.\n",
    "\n",
    "Then, please provide your email address and obtain a submission token on the grader’s submission page in coursera, then execute the subsequent cells\n",
    "\n",
    "### Note: We've changed the grader in this assignment and will do so for the others soon since it gives less errors\n",
    "This means you can directly submit your solutions from this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-24 12:16:29--  https://raw.githubusercontent.com/IBM/coursera/master/rklib.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2540 (2.5K) [text/plain]\n",
      "rklib.py: Permission denied\n",
      "\n",
      "Cannot write to ‘rklib.py’ (Success).\n"
     ]
    }
   ],
   "source": [
    "!rm -f rklib.py\n",
    "!wget https://raw.githubusercontent.com/IBM/coursera/master/rklib.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rklib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrklib\u001b[39;00m \u001b[39mimport\u001b[39;00m submitAll\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      4\u001b[0m key \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSuy4biHNEeimFQ479R3GjA\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rklib'"
     ]
    }
   ],
   "source": [
    "from rklib import submitAll\n",
    "import json\n",
    "\n",
    "key = \"Suy4biHNEeimFQ479R3GjA\"\n",
    "email = \" zulqar445ali@gmail.com\"\n",
    "token = \"TA9QDVqMuHH1Mha1\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m parts_data \u001b[39m=\u001b[39m {}\n\u001b[0;32m----> 2\u001b[0m parts_data[\u001b[39m\"\u001b[39m\u001b[39mFWMEL\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps(min_temperature)\n\u001b[1;32m      3\u001b[0m parts_data[\u001b[39m\"\u001b[39m\u001b[39m3n3TK\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps(mean_temperature)\n\u001b[1;32m      4\u001b[0m parts_data[\u001b[39m\"\u001b[39m\u001b[39mKD3By\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mdumps(max_temperature)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "parts_data = {}\n",
    "parts_data[\"FWMEL\"] = json.dumps(min_temperature)\n",
    "parts_data[\"3n3TK\"] = json.dumps(mean_temperature)\n",
    "parts_data[\"KD3By\"] = json.dumps(max_temperature)\n",
    "parts_data[\"06Zie\"] = json.dumps(sd_temperature)\n",
    "parts_data[\"Qc8bI\"] = json.dumps(skew_temperature)\n",
    "parts_data[\"LoqQi\"] = json.dumps(kurtosis_temperature)\n",
    "parts_data[\"ehNGV\"] = json.dumps(correlation_temperature)\n",
    "\n",
    "\n",
    "\n",
    "submitAll(email, token, key, parts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
